Here is the equivalent PySpark code for your SQL query:

```python
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.window import Window

# Initialize Spark Session
spark = SparkSession.builder.getOrCreate()

# Load tables
inol = spark.table("ddwh01_dw.td_inol_invoice_orders_lines")
delta = spark.table("ddwh02_dim.twrk_pvan_delta_flash_leg")
vele = spark.table("ddwh01_dw.tm_vele_vehicle_legacy")
sops = spark.table("ddwh01_dw.td_sops_sales_order_position")
slor = spark.table("ddwh01_dw.tt_slor_sales_order")
inoh = spark.table("ddwh01_dw.tt_inoh_invoice_orders_header")
tdim_pvan = spark.table("tdim_pvan_vehicle")  # Assuming this is a table in your database

# Define window function to get max sales order position id
window = Window.partitionBy(sops['sops_cd_sales_order'], sops['sops_cd_legacy_mkt'], sops['sops_cd_dealer_code'], 
                            sops['sops_cd_source_order'], sops['sops_cd_source_order_sequence'], sops['sops_cd_source_system'], 
                            sops['sops_cd_product_line'], sops['sops_id_vele_vcl_legacy'])

# Join tables and apply conditions
df = inol.join(delta, on=inol["inol_id_vele_vcl_legacy"] == delta["vele_id_vehicle_legacy_id_pk"]) \
        .join(vele, on=inol["inol_id_vele_vcl_legacy"] == vele["vele_id_vehicle_legacy_id_pk"], how='left') \
        .join(sops.withColumn("max_id_sops", F.max('sops_id_sales_order_pos').over(window)), 
              ((inol["inol_cd_sales_order"] == sops["sops_cd_sales_order"]) & 
               (inol["inol_cd_legacy_mkt"] == sops["sops_cd_legacy_mkt"]) & 
               (inol["inol_cd_dealer_code"] == sops["sops_cd_dealer_code"]) & 
               (inol["inol_cd_source_order"] == sops["sops_cd_source_order"]) & 
               (inol["inol_cd_source_order_sequence"] == sops["sops_cd_source_order_sequence"]) & 
               (inol["inol_cd_source_system"] == sops["sops_cd_source_system"]) & 
               (inol["inol_cd_product_line"] == sops["sops_cd_product_line"]) & 
               (inol["inol_id_vele_vcl_legacy"] == sops["sops_id_vele_vcl_legacy"]) & 
               (sops['sops_id_sales_order_pos'] == sops['max_id_sops'])), how='left') \
        .join(slor, on=sops["sops_id_slor_sales_order"] == slor["slor_id_sales_order"], how='left') \
        .join(inoh, on=inol["inol_id_inoh_invoice_orders_he"] == inoh["inoh_id_invoice_orders_header"], how='left') \
        .join(tdim_pvan, on=tdim_pvan["pvan_id_vcl"] == inol["inol_id_vele_vcl_legacy"], how='left') \
        .where(inol['inol_fl_delete_line'].isNull())
```

Please note that this code assumes the existence of a 'tdim_pvan_vehicle' table in your database. If it doesn't exist, you need to replace it with the appropriate PySpark dataframe or table. Also, please adjust column names and transformations according to your actual schema and requirements.
