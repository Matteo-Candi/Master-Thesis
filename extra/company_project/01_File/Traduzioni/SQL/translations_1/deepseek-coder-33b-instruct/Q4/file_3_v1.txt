Here is the translation of your SQL query to PySpark:

```python
# Import necessary functions and types
from pyspark.sql import SparkSession, Window
import pyspark.sql.functions as F

# Create a Spark session
spark = SparkSession.builder \
    .appName('SQL to PySpark Translation') \
    .getOrCreate()

# Read the tables from SQL
EKPO_PURCH_DOC_ITEM_GPP = spark.table("ddwh01_dw.TT_EKPO_PURCH_DOC_ITEM_GPP")
PLANT_BRANCHES = spark.table("ddwh01_dw.TD_PLBR_PLANT_BRANCHES")
SUPM_SUPPLIER_GPP = spark.table("ddwh01_dw.TM_SUPM_SUPPLIER_GPP")
EKKO_PURCH_DOC_HEADER_GPP = spark.table("ddwh01_dw.TT_EKKO_PURCH_DOC_HEADER_GPP")

# Create the subquery for AGCE
AGCE = EKPO_PURCH_DOC_ITEM_GPP \
    .join(PLANT_BRANCHES, (EKPO_PURCH_DOC_ITEM_GPP.EKPO_CD_PLANT == PLANT_BRANCHES.PLBR_CD_PLANT) 
          & PLANT_BRANCHES.PLBR_DS_PLANT_SEGMENT.isin(['AG', 'CE', '-1', 'AM', 'AS']) 
          & ~(EKPO_PURCH_DOC_ITEM_GPP.EKPO_CD_PLANT.isin(['FA01', 'FB01', 'IA01'])) \
          & ~EKPO_PURCH_DOC_ITEM_GPP.EKPO_CD_PLANT.like('00%'), "inner") \
    .select(EKPO_PURCH_DOC_ITEM_GPP.EKPO_CD_PO_NBR)

# Create the subquery for SUPM
SUPM = SUPM_SUPPLIER_GPP \
    .withColumn("Supplier_Type", F.when(F.length(SUPM_SUPPLIER_GPP.SUPM_CD_COMP_TRADING_PARTNER) > 1, "Intercompany") \
                       .otherwise("Third Party")) \
    .select(SUPM_SUPPLIER_GPP.SUPM_CD_ACCOUNT_NBR, SUPM_SUPPLIER_GPP.Supplier_Type)

# Join the tables and select the required columns
df = EKKO_PURCH_DOC_HEADER_GPP \
    .join(AGCE, EKKO_PURCH_DOC_HEADER_GPP.EKKO_CD_PURCH_DOC_NBR == AGCE.EKPO_CD_PO_NBR, "inner") \
    .join(SUPM, EKKO_PURCH_DOC_HEADER_GPP.EKKO_CD_VENDOR_ACCOUNT_NBR == SUPM.SUPM_CD_ACCOUNT_NBR, "left") \
    .select("EKKO_DT_LAST_MODIFY", "EKKO_ID_PURCH_DOC_HEADER_GPP", "EKKO_CD_PURCH_DOC_NBR", 
            "EKKO_NR_TOT_VAL_OF_RELEASE", "EKKO_DT_PURCH_DOC_DATE", "EKKO_DT_RECORD_CREATED_ON", 
            "EKKO_CD_PURCH_GRP", "EKKO_FL_TAKE_PURCH_GRP", "EKKO_CD_VENDOR_ACCOUNT_NBR", 
            "EKKO_CD_DOC_PROCESS_STATE", "EKKO_CD_PURCH_DOC_CATEGORY", "EKKO_CD_PURCH_DOCTYPE", 
            "EKKO_DT_CONTRACT_START_DATE", "EKKO_NR_CHG_TIMESTAMP", "EKKO_DT_CONTRACT_END_DATE", 
            "EKKO_CD_PURCH_DOC_STATUS", "EKKO_NR_CURRENCY_KEY", "EKKO_CD_PURCH_AGREEMENT_NBR", 
            "EKKO_CD_OBJ_CREATED_BY", "EKKO_CD_INCOTERMS_PART_1", "EKKO_CD_INCOTERMS_PART_2", 
            "EKKO_CD_TERMS_OF_PAYMENT_1", "EKKO_CD_PURCH_ORG_1", "EKKO_CD_COMPANY_CODE", 
            "EKKO_CD_REMARK_1", F.year(F.col("EKKO_DT_PURCH_DOC_DATE")).alias("EKKO_NR_PURCH_DOC_YEAR"), 
            F.lit('').alias("PLBR_DS_PLANT_SEGMENT"), "EKKO_CD_PURCH_DOCTYPE", "EKKO_CD_VENDOR_ACCOUNT_NBR", 
            F.when(F.col("EKKO_DT_CONTRACT_END_DATE").isNotNull(), F.year(F.col("EKKO_DT_CONTRACT_END_DATE"))) \
                .otherwise(F.year(F.col("EKKO_DT_RECORD_CREATED_ON"))).alias("EKKO_DT_RETENTION_YEAR"), 
            F.when(F.col("EKKO_CD_PURCH_DOC_NBR").isNotNull(), F.year(F.col("EKKO_DT_PURCH_DOC_DATE"))) \
                .otherwise('').alias("ODAG_PODOCUMENTYEAR"), 
            F.expr("substr('0000000000' || EKKO_CD_PURCH_DOC_NBR, -10)").alias("EKKO_CD_PURCH_DOC_NBR10"), 
            "Supplier_Type")
```
This code assumes that the tables `TT_EKPO_PURCH_DOC_ITEM_GPP`, `TD_PLBR_PLANT_BRANCHES`, `TM_SUPM_SUPPLIER_GPP` and `TT_EKKO_PURCH_DOC_HEADER_GPP` are accessible in the PySpark environment. If they're not, you might need to read them from a different source (e.g., CSV files).
