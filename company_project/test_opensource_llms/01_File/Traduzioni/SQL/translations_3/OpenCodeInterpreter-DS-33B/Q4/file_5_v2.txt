
The PySpark equivalent code for the given SQL query would be:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import to_number, from_unixtime, coalesce

# Creating a spark session
spark = SparkSession.builder \
    .appName('SQL to PySpark') \
    .getOrCreate()

# Read the table in the DWH
inol = spark.read.table("ddwh01_dw.td_inol_invoice_orders_lines")
vele = spark.read.table("ddwh02_dim.twrk_pvan_delta_flash_legacy")
slor = spark.read.table("ddwh01_dw.tt_slor_sales_order")
inoh = spark.read.table("ddwh01_dw.tt_inoh_invoice_orders_header")
tdim_pvan = spark.read.table("tdim_pvan_vehicle")

# Perform the join operations and transformations
result = inol \
    .join(vele, on='inol_id_vele_vcl_legacy') \
    .join(slor, on='sops_id_slor_sales_order') \
    .join(inoh, on='inoh_id_invoice_orders_header') \
    .join(tdim_pvan, on='pvan_id_vcl') \
    .select('-1', 
              '-1', 
              to_number(from_unixtime('inol_dt_document_date'), 'YYYY')) as fbil_nr_accg_doc_fiscal_year, 
              'inol_cd_document_number' as fbil_cd_cmm_doc_code, 
              'inol_cd_progressive_order' as fbil_cd_cmm_doc_pos_code, 
              coalesce('inol_id_vele_vcl_legacy', -1) as fbil_id_pvan_vcl, 
              'inol_cd_sales_order' as fbil_cd_sls_ord_code, 
              'inol_cd_progressive_order' as fbil_cd_sls_ord_pos_code, 
              coalesce('inol_id_inoh_invoice_orders_he', -1) as fbil_id_cmm_doc_id, 
              coalesce('inol_id_invoice_orders_lines', -1) as fbil_id_cmm_doc_pos_id, 
              'inol_cd_document_type' as fbil_id_bdty_billing_doc_tp, 
              coalesce(to_number(from_unixtime('inol_dt_document_date'), 'J'), 5373484) as fbil_id_time_cmm_doc_date, 
              coalesce('inoh_id_cust_customer', -1) as fbil_id_cust_payer, 
              coalesce('inol_id_cust_customer', -1) as fbil_id_cust_sold, 
              '-2' as fbil_id_cuty_customer_type, 
              coalesce('vele_id_ledi_legacy_division', -1) as fbil_id_divi_division, 
              coalesce('vele_id_hmup_plank_key', -1) as fbil_id_hmup_hmu_product, 
              coalesce('pvan_id_prat_pr_attributes', -1) as fbil_id_prat_pr_attributes, 
              coalesce('pvan_id_prpp_prp_product', -1) as fbil_id_prp_prpp_product, 
              '5373484' as fbil_id_time_return_date, 
              coalesce(to_number(from_unixtime('slor_dt_document_date'), 'J'), 5373484) as fbil_id_time_sales_order_date, 
              coalesce('slor_id_sales_order', -1) as fbil_id_sls_ord_id, 
              coalesce('sops_id_sales_order_pos', -1) as fbil_id_sls_ord_pos_id, 
              coalesce(to_number(from_unixtime('vele_dt_dealer_invoice_date'), 'J'), 5373484) as fbil_id_time_start_billing_dat, 
              'inoh_cd_currency_code' as fbil_cd_currency, 
              '1' as fbil_id_batch_id, 
              None as fbil_fl_itc_flg, 
              coalesce('inoh_id_cust_customer', -1) as fbil_id_cust_payee, 
              coalesce('inol_id_cust_customer', -1) as fbil_id_cust_billtoparner, 
              coalesce('inoh_id_cust_customer', -1) as fbil_id_cust_saleschannel, 
              coalesce('inol_id_cust_customer', -1) as fbil_id_cust_soldtoparner, 
              coalesce('inoh_id_cust_customer', -1) as fbil_id_cust_ownership_group, 
              '-2' as fbil_id_addr_ownership_group, 
              'inol_cd_business_area_code' as fbil_cd_business_area_code, 
              'slor_cd_codice_resa' as fbil_cd_codice_resa, 
              'inol_cd_dealer_code' as fbil_cd_dlr_code, 
              'inoh_cd_dealer_order_reference' as fbil_cd_dlr_ord_ref, 
              'inol_cd_dealer_order_type' as fbil_cd_dlr_ord_tp, 
              'inol_cd_document_number' as fbil_cd_doc_nbr, 
              'inol_cd_document_type' as fbil_cd_doc_tp, 
              'inol_cd_draft_number' as fbil_cd_draft_inv_code, 
              'inol_cd_engine_number' as fbil_cd_eng_nbr, 
              'inol_cd_machine_type' as fbil_cd_machine_tp, 
              'inol_cd_legacy_mkt' as fbil_cd_market_legacy, 
              'inol_cd_tyres_price_1' as fbil_cd_price_tyres_1, 
              'inol_cd_tyres_price_2' as fbil_cd_price_tyres_2, 
              'inol_cd_price_list' as fbil_cd_price_list, 
              'inol_cd_tech_type_price' as fbil_cd_price_tech_tp, 
              'inol_cd_product_class' as fbil_cd_product_cl, 
              'inol_cd_product_line' as fbil_cd_product_line, 
              'inol_cd_product_type' as fbil_cd_product_tp, 
              'inol_cd_progressive_order' as fbil_cd_progressive_ord, 
              'inol_cd_reference_document_num' as fbil_cd_ref_doc_nbr, 
              'inol_cd_serial_number' as fbil_cd_serial_nbr, 
              'inol_cd_shipment_progressive' as fbil_cd_shipment_progressive, 
              'inol_cd_sales_order' as fbil_cd_sls_ord, 
              'inol_cd_source_order' as fbil_cd_source_ord, 
              'inol_cd_source_order_sequence' as fbil_cd_source_ord_sequence, 
              'inol_cd_standard_card_model' as fbil_cd_std_card_mdl, 
              'inol_cd_standard_card_tyres' as fbil_cd_std_card_tyres, 
              'inol_cd_standard_card_version' as fbil_cd_std_card_vrs, 
              'inol_cd_tech_tp' as fbil_cd_tech_tp, 
              '5373484' as fbil_id_time_itc_inv_date, 
              'inol_fl_invoice_reject_reason' as fbil_fl_inv_reject_reason, 
              coalesce(to_number(from_unixtime('inol_dt_draft_date'), 'J'), 5373484) as fbil_id_time_draft_inv_date, 
              coalesce('inoh_id_mark_std_legacy_mkt', -1) as fbil_id_coun_country, 
              'inol_cd_source_system' as fbil_cd_source_system, 
              1 if inol.inol_cd_document_type == '1' else 0 as fbil_in_wholesale_qty, 
              1 - (1 if inol.inol_cd_document_type == '6' else 0) as fbil_in_wholesale_net_qty 

# Write the result to a new table
result.write \
    .mode('append') \
    .table("ddwh02_sm.twrk_fbil_billing_l_flash")
```
The above code is intended to be executed in an environment with PySpark support. You need to replace 'ddwh01_dw' and 'ddwh02_dim' with the actual table names and paths that correspond to your database schema, respectively. Also note that this script assumes you have a valid Spark Session and you should install necessary libraries (like pyspark) before executing it.